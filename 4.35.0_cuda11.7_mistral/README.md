# Huggingface transformers (QLoRA)

Docker image for [Huggingface transformers](https://github.com/huggingface/transformers) 4.35.0
that contains support for [Mistral 7B](https://gathnex.medium.com/mistral-7b-fine-tuning-a-step-by-step-guide-52122cdbeca8).

Uses PyTorch 2.0.1, CUDA 11.7

## Quick start

### Inhouse registry

* Log into registry using *public* credentials:

  ```bash
  docker login -u public -p public public.aml-repo.cms.waikato.ac.nz:443 
  ```

* Create the following directories:

  ```bash
  mkdir cache triton
  ```

* Launch docker container

  ```bash
  docker run \
    -u $(id -u):$(id -g) -e USER=$USER \
    --gpus=all \
    --shm-size 8G \
    --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/cache:/.cache \
    -v `pwd`/triton:/.triton \
    -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral
  ```

### Docker hub
  
* Create the following directories:

  ```bash
  mkdir cache triton
  ```

* Launch docker container

  ```bash
  docker run \
    -u $(id -u):$(id -g) -e USER=$USER \
    --gpus=all \
    --shm-size 8G \
    --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/cache:/.cache \
    -v `pwd`/triton:/.triton \
    -it waikatodatamining/pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral
  ```

### Build local image

* Build the image from Docker file (from within /path_to/huggingface-transformers/4.35.0_cuda11.7_mistral)

  ```bash
  docker build -t hf_mistral .
  ```
  
* Run the container

  ```bash
  docker run --gpus=all --shm-size --net=host 8G -v /local/dir:/container/dir -it hf_mistral
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container


## Publish images

### Build

```bash
docker build -t pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral .
```

### Inhouse registry  
  
* Tag

  ```bash
  docker tag \
    pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral \
    public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral
  ```
  
* Push

  ```bash
  docker push public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public-push.aml-repo.cms.waikato.ac.nz:443
  ```

### Docker hub  
  
* Tag

  ```bash
  docker tag \
    pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral \
    waikatodatamining/pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral
  ```
  
* Push

  ```bash
  docker push waikatodatamining/pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login
  ```


### Requirements

```bash
docker run --rm --pull=always \
  -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.35.0_cuda11.7_mistral \
  pip freeze > requirements.txt
```


## Permissions

When running the docker container as regular use, you will want to set the correct
user and group on the files generated by the container (aka the user:group launching
the container):

```bash
docker run -u $(id -u):$(id -g) -e USER=$USER ...
```

## Scripts

* `mistral_finetune` - for finetuning a Mistral model (calls `/opt/mistral/mistral_finetune.py`)
* `mistral_interact` - lets the user interact with a Mistral model (calls `/opt/mistral/mistral_interact.py`)
* `mistral_redis` - processing of JSON prompts via Redis (calls `/opt/mistral/mistral_redis.py`)


### Prompt format

```json
{
  "prompt": "the prompt text."
}
```
