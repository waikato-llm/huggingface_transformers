# Huggingface transformers (Llama2)

Docker image for [Huggingface transformers](https://github.com/huggingface/transformers) 4.31.0
that contains support for [Llama2](https://www.datacamp.com/tutorial/fine-tuning-llama-2).

Uses PyTorch 2.0.1, CUDA 11.7

## Quick start

### Inhouse registry

* Log into registry using *public* credentials:

  ```bash
  docker login -u public -p public public.aml-repo.cms.waikato.ac.nz:443 
  ```

* Create the following directories:

  ```bash
  mkdir cache triton
  ```

* Launch docker container

  ```bash
  docker run \
    -u $(id -u):$(id -g) -e USER=$USER \
    --gpus=all \
    --shm-size 8G \
    --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/cache:/.cache \
    -v `pwd`/triton:/.triton \
    -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit
  ```

### Docker hub
  
* Create the following directories:

  ```bash
  mkdir cache triton
  ```

* Launch docker container

  ```bash
  docker run \
    -u $(id -u):$(id -g) -e USER=$USER \
    --gpus=all \
    --shm-size 8G \
    --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/cache:/.cache \
    -v `pwd`/triton:/.triton \
    -it waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit
  ```

### Build local image

* Build the image from Docker file (from within /path_to/huggingface-transformers/4.31.0_cuda11.7_llama2_8bit)

  ```bash
  docker build -t hf_llama2 .
  ```
  
* Run the container

  ```bash
  docker run --gpus=all --shm-size --net=host 8G -v /local/dir:/container/dir -it hf_llama2
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container


## Publish images

### Build

```bash
docker build -t pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit .
```

### Inhouse registry  
  
* Tag

  ```bash
  docker tag \
    pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit \
    public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit
  ```
  
* Push

  ```bash
  docker push public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public-push.aml-repo.cms.waikato.ac.nz:443
  ```

### Docker hub  
  
* Tag

  ```bash
  docker tag \
    pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit \
    waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit
  ```
  
* Push

  ```bash
  docker push waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login
  ```


### Requirements

```bash
docker run --rm --pull=always \
  -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2_8bit \
  pip freeze > requirements.txt
```


## Permissions

When running the docker container as regular use, you will want to set the correct
user and group on the files generated by the container (aka the user:group launching
the container):

```bash
docker run -u $(id -u):$(id -g) -e USER=$USER ...
```

## Scripts

* `llama2_finetune` - for finetuning a Llama2 model (calls `/opt/llama2/llama2_finetune.py`)
* `llama2_interact` - lets the user interact with a Llama2 model (calls `/opt/llama2/llama2_interact.py`)
* `llama2_redis` - processing of JSON prompts via Redis (calls `/opt/llama2/llama2_redis.py`)


### Prompt format

```json
{
  "prompt": "the prompt text."
}
```
